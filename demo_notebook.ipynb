{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Demo: Medical Concept Embedding Evaluation\n",
        "\n",
        "This notebook demonstrates a minimal, self-contained evaluation pipeline for medical concept embeddings. It shows:\n",
        "- Creating a small sample dataset (queries + candidate synonyms + relevance)\n",
        "- Using SentenceTransformers or OpenAI Embeddings as embedding providers (if available)\n",
        "- Computing candidate embeddings, ranking by cosine similarity\n",
        "- Computing NDCG@k, MSE, and Spearman correlation per query and aggregated results\n",
        "\n",
        "Notes:\n",
        "- If you plan to run OpenAI embeddings, set the environment variable OPENAI_API_KEY first.\n",
        "- The notebook is intentionally self-contained so you can run it even if you don't have the rest of the repo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install",
      "metadata": {},
      "source": [
        "## Install dependencies (run in a notebook cell)\n",
        "\n",
        "If you don't have the libraries already, uncomment and run the following cell. This may take a few minutes for the sentence-transformers model download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -q sentence-transformers openai scipy scikit-learn pandas numpy\n",
        "# If you use Pinecone elsewhere: !pip install -q pinecone-client\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utils",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from typing import List, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    SentenceTransformer = None\n",
        "\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "def dcg_at_k(relevances, k):\n",
        "    relevances = np.asarray(relevances)[:k]\n",
        "    if relevances.size == 0:\n",
        "        return 0.0\n",
        "    discounts = np.log2(np.arange(2, relevances.size + 2))\n",
        "    gains = (2 ** relevances - 1) / discounts\n",
        "    return float(np.sum(gains))\n",
        "\n",
        "def ndcg_at_k_per_query(true_rels, pred_scores, k):\n",
        "    if len(true_rels) == 0:\n",
        "        return 0.0\n",
        "    order = np.argsort(pred_scores)[::-1]\n",
        "    pred_ordered_rels = np.asarray(true_rels)[order]\n",
        "    dcg = dcg_at_k(pred_ordered_rels.tolist(), k)\n",
        "    ideal_order = np.sort(true_rels)[::-1]\n",
        "    idcg = dcg_at_k(ideal_order.tolist(), k)\n",
        "    return 0.0 if idcg == 0.0 else dcg / idcg\n",
        "\n",
        "class SentenceTransformersEmbedder:\n",
        "    def __init__(self, model_name_or_path: str):\n",
        "        if SentenceTransformer is None:\n",
        "            raise RuntimeError(\"sentence-transformers not installed\")\n",
        "        print(f\"Loading sentence-transformers model: {model_name_or_path}\")\n",
        "        self.model = SentenceTransformer(model_name_or_path)\n",
        "\n",
        "    def encode(self, texts: List[str], batch_size: int = 64, show_progress_bar: bool = True) -> np.ndarray:\n",
        "        emb = self.model.encode(texts, batch_size=batch_size, show_progress_bar=show_progress_bar)\n",
        "        arr = np.asarray(emb, dtype=np.float32)\n",
        "        norms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        arr = arr / norms\n",
        "        return arr\n",
        "\n",
        "class OpenAIEmbedder:\n",
        "    def __init__(self, model_name: str = \"text-embedding-3-small\"):\n",
        "        if openai is None:\n",
        "            raise RuntimeError(\"openai package not installed\")\n",
        "        self.model = model_name\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"OPENAI_API_KEY environment variable not set\")\n",
        "        openai.api_key = api_key\n",
        "\n",
        "    def encode(self, texts: List[str], batch_size: int = 64, show_progress_bar: bool = False) -> np.ndarray:\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i : i + batch_size]\n",
        "            resp = openai.Embedding.create(model=self.model, input=batch)\n",
        "            batch_embs = [d['embedding'] for d in resp['data']]\n",
        "            embeddings.extend(batch_embs)\n",
        "            time.sleep(0.1)\n",
        "        arr = np.asarray(embeddings, dtype=np.float32)\n",
        "        norms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        arr = arr / norms\n",
        "        return arr\n",
        "\n",
        "def build_embedding_encoder(model_name: str, model_type: str):\n",
        "    mt = (model_type or \"\").lower()\n",
        "    if mt in {\"openai\", \"openai-embeddings\"}:\n",
        "        return OpenAIEmbedder(model_name)\n",
        "    if mt in {\"sentence-transformers\", \"sentencetransformer\", \"hf\", \"huggingface\"}:\n",
        "        return SentenceTransformersEmbedder(model_name)\n",
        "    # fallback: assume model_name is a sentence-transformers model path\n",
        "    return SentenceTransformersEmbedder(model_name)\n",
        "\n",
        "def evaluate(df: pd.DataFrame, encoder: Any, ndcg_k: int = 10, batch_size: int = 64, query_id_col: str = \"query_id\", query_col: str = \"query_text\", candidate_col: str = \"candidate_text\", relevance_col: str = \"relevance\"):\n",
        "    # unique queries\n",
        "    queries = df[[query_id_col, query_col]].drop_duplicates(subset=[query_id_col]).set_index(query_id_col)[query_col].to_dict()\n",
        "    candidates = df[[candidate_col]].drop_duplicates().reset_index(drop=True)[candidate_col].tolist()\n",
        "    cand_idx = {text: i for i, text in enumerate(candidates)}\n",
        "    query_texts = list(queries.values())\n",
        "    query_ids = list(queries.keys())\n",
        "    query_emb = encoder.encode(query_texts, batch_size=batch_size)\n",
        "    candidate_emb = encoder.encode(candidates, batch_size=batch_size)\n",
        "    ndcgs, mses, spearmans = [], [], []\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        qvec = query_emb[i]\n",
        "        subset = df[df[query_id_col] == qid]\n",
        "        if subset.empty:\n",
        "            continue\n",
        "        cand_texts = subset[candidate_col].tolist()\n",
        "        true_rels = subset[relevance_col].astype(float).tolist()\n",
        "        cand_indices = [cand_idx[t] for t in cand_texts]\n",
        "        cand_vecs = candidate_emb[cand_indices]\n",
        "        pred_scores = (cand_vecs @ qvec).astype(float).tolist()\n",
        "        true_rels_arr = np.asarray(true_rels, dtype=float)\n",
        "        if true_rels_arr.max() > 0:\n",
        "            true_norm = (true_rels_arr - true_rels_arr.min()) / (true_rels_arr.max() - true_rels_arr.min())\n",
        "        else:\n",
        "            true_norm = true_rels_arr\n",
        "        mse = float(mean_squared_error(true_norm.tolist(), pred_scores))\n",
        "        mses.append(mse)\n",
        "        try:\n",
        "            if np.unique(true_rels_arr).size > 1:\n",
        "                rho, _ = spearmanr(true_rels_arr, pred_scores)\n",
        "                rho = 0.0 if np.isnan(rho) else float(rho)\n",
        "            else:\n",
        "                rho = 0.0\n",
        "        except Exception:\n",
        "            rho = 0.0\n",
        "        spearmans.append(rho)\n",
        "        ndcg = ndcg_at_k_per_query(true_rels, pred_scores, ndcg_k)\n",
        "        ndcgs.append(ndcg)\n",
        "    results = {\n",
        "        f\"ndcg@{ndcg_k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
        "        \"mse\": float(np.mean(mses)) if mses else 0.0,\n",
        "        \"spearman\": float(np.mean(spearmans)) if spearmans else 0.0,\n",
        "        \"queries_evaluated\": len(ndcgs),\n",
        "    }\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sample-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small sample dataset\n",
        "data = [\n",
        "    # query_id, query_text, candidate_text, relevance (0-3)\n",
        "    (\"q1\", \"acute myocardial infarction\", \"heart attack\", 3),\n",
        "    (\"q1\", \"acute myocardial infarction\", \"myocardial infarct\", 2),\n",
        "    (\"q1\", \"acute myocardial infarction\", \"chest pain\", 1),\n",
        "    (\"q1\", \"acute myocardial infarction\", \"diabetes\", 0),\n",
        "    (\"q2\", \"hypertension\", \"high blood pressure\", 3),\n",
        "    (\"q2\", \"hypertension\", \"HTN\", 2),\n",
        "    (\"q2\", \"hypertension\", \"elevated BP\", 2),\n",
        "    (\"q2\", \"hypertension\", \"headache\", 0),\n",
        "    (\"q3\", \"type 2 diabetes mellitus\", \"T2DM\", 3),\n",
        "    (\"q3\", \"type 2 diabetes mellitus\", \"adult-onset diabetes\", 2),\n",
        "    (\"q3\", \"type 2 diabetes mellitus\", \"insulin-dependent diabetes\", 0),\n",
        "]\n",
        "df = pd.DataFrame(data, columns=[\"query_id\", \"query_text\", \"candidate_text\", \"relevance\"]) \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-sentence-transformers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with sentence-transformers (if installed)\n",
        "try:\n",
        "    encoder = build_embedding_encoder(\"all-mpnet-base-v2\", \"sentence-transformers\")\n",
        "    print(\"Computing evaluation with sentence-transformers...\")\n",
        "    res = evaluate(df, encoder, ndcg_k=3, batch_size=32)\n",
        "    print(\"Results (sentence-transformers):\", res)\n",
        "except Exception as e:\n",
        "    print(\"Sentence-transformers evaluation skipped or failed:\", str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-openai",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with OpenAI embeddings if OPENAI_API_KEY is set and openai is installed\n",
        "try:\n",
        "    if os.environ.get(\"OPENAI_API_KEY\") and openai is not None:\n",
        "        # choose a model available to your account, e.g., 'text-embedding-3-small'\n",
        "        encoder_oa = build_embedding_encoder(\"text-embedding-3-small\", \"openai\")\n",
        "        print(\"Computing evaluation with OpenAI embeddings (this will call the API)...\")\n",
        "        res_oa = evaluate(df, encoder_oa, ndcg_k=3, batch_size=8)\n",
        "        print(\"Results (OpenAI):\", res_oa)\n",
        "    else:\n",
        "        print(\"Skipping OpenAI evaluation: either OPENAI_API_KEY not set or openai package missing.\")\n",
        "except Exception as e:\n",
        "    print(\"OpenAI evaluation failed:\", str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Replace the sample DataFrame with your evaluation dataset (CSV/Parquet/JSONL) and ensure columns match.\n",
        "- Use the same encoder building pattern to evaluate other models (HF Hub model names or OpenAI models).\n",
        "- If you want to persist vectors to Pinecone, reuse the candidate embeddings and upsert (take care to use stable IDs).\n",
        "- For large datasets, add batching and caching of candidate embeddings to avoid recomputing.\n",
        "\n",
        "This completes the demo notebook. Save/download it and adapt to your dataset and environment."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}